{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.7),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 40K images\n",
    "train_ds = CIFAR10(\n",
    "    \"data/\", train=True, download=True, transform=train_transform\n",
    ")\n",
    "\n",
    "# 10K images\n",
    "test_ds = CIFAR10(\n",
    "    \"data/\", train=False, download=True, transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train dataset into train and validation datasets\n",
    "validation_size = 10000\n",
    "train_size= len(train_ds) - validation_size\n",
    "train_ds, validation_ds = random_split(train_ds, [train_size, validation_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 54\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "validation_dl = DataLoader(validation_ds, batch_size=batch_size, shuffle=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images,_ in train_dl:\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # make_grid adds a grid around and between images 2px of width\n",
    "    imgs_grid = make_grid(images, nrow=16) # 16 rows, 4 columns (16*4 = 64 images)\n",
    "    \n",
    "    # use permute() to move channel from the first\n",
    "    # to the last dimension (required by matplotlib)\n",
    "    plt.imshow(imgs_grid.permute(1, 2, 0))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images,_ in test_dl:\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # make_grid adds a grid around and between images 2px of width\n",
    "    imgs_grid = make_grid(images, nrow=16) # 16 rows, 4 columns (16*4 = 64 images)\n",
    "    \n",
    "    # use permute() to move channel from the first\n",
    "    # to the last dimension (required by matplotlib)\n",
    "    plt.imshow(imgs_grid.permute(1, 2, 0))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images,_ in validation_dl:\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # make_grid adds a grid around and between images 2px of width\n",
    "    imgs_grid = make_grid(images, nrow=16) # 16 rows, 4 columns (16*4 = 64 images)\n",
    "    \n",
    "    # use permute() to move channel from the first\n",
    "    # to the last dimension (required by matplotlib)\n",
    "    plt.imshow(imgs_grid.permute(1, 2, 0))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the VGG-16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(25088, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.conv1_1(x))\n",
    "        y = F.relu(self.conv1_2(y))\n",
    "        y = self.maxpool(y)\n",
    "\n",
    "        y = F.relu(self.conv2_1(y))\n",
    "        y = F.relu(self.conv2_2(y))\n",
    "        y = self.maxpool(y)\n",
    "\n",
    "        y = F.relu(self.conv3_1(y))\n",
    "        y = F.relu(self.conv3_2(y))\n",
    "        y = F.relu(self.conv3_3(y))\n",
    "        y = self.maxpool(y)\n",
    "        \n",
    "        y = F.relu(self.conv4_1(y))\n",
    "        y = F.relu(self.conv4_2(y))\n",
    "        y = F.relu(self.conv4_3(y))\n",
    "        y = self.maxpool(y)\n",
    "\n",
    "        y = F.relu(self.conv5_1(y))\n",
    "        y = F.relu(self.conv5_2(y))\n",
    "        y = F.relu(self.conv5_3(y))\n",
    "        y = self.maxpool(y)\n",
    "\n",
    "        # TODO check the shape of y before and after reshape\n",
    "        y = y.reshape(y.shape[0], -1)\n",
    "\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = F.dropout(y, 0.5) # TODO check the impacts of this dropout\n",
    "\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = F.dropout(y, 0.5) # TODO check the impacts of this dropout\n",
    "\n",
    "        y = self.fc3(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()\n",
    "model = model.to(device=device)\n",
    "\n",
    "load_model = True\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    \n",
    "    loss_ep = 0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_dl):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_ep += loss.item()\n",
    "    print(f\"Loss in epoch {epoch} :::: {loss_ep/len(train_dl)}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        for batch_idx, (data, targets) in enumerate(validation_dl):\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            scores = model.forward(data)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == targets).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(\n",
    "            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "torch.save(model.state_dict(), \"vgg16_cifar.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()\n",
    "model.load_state_dict(torch.load(\"vgg16_cifar.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "num_samples = 0\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device=device)\n",
    "\n",
    "for batch_idx, (inputs, outputs) in enumerate(test_dl):\n",
    "    inputs = inputs.to(device=device)\n",
    "    outputs = outputs.to(device=device)\n",
    "\n",
    "    scores = model(inputs)\n",
    "    \n",
    "    #print(scores)\n",
    "    break\n",
    "    _, predictions = scores.max(1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "809fe4aa185832e5bfac25d646fe0f503bb060d383ce9d20e9bb0276104f2971"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
